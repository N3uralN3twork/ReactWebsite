{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca373651-70a0-4095-bae2-88260c313d02",
      "metadata": {},
      "source": [
        "# XGBoost\n",
        "\n",
        "Matthias Quinn, Matthew Brigham  \n",
        "2021-12-08\n",
        "\n",
        "# **The XGBoost Boosting Algorithm and an Application to the Forest Cover Type Dataset**\n",
        "\n",
        "**Matthias Quinn, Matthew Brigham**\n",
        "\n",
        "**Fall 2021**\n",
        "\n",
        "# Abstract\n",
        "\n",
        "This project will focus on the famous XGBoost system and its application\n",
        "on a moderately large dataset. Starting with a history of the system,\n",
        "then exploring the algorithm itself, and finally ending with an\n",
        "application to forest cover types, this project will hopefully provide a\n",
        "framework on which to base further research and applications.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Tree-based methods are popular due to their superior performance on\n",
        "tabular data. They work by repeatedly dividing the predictor space into\n",
        "regions in order to make predictions for both classification and\n",
        "regression problems. The most basic type of tree is the decision tree.\n",
        "The decision tree is simple to implement and to interpret. However, it\n",
        "usually doesn’t perform as well as other supervised methods in terms of\n",
        "accuracy. Bagging, boosting, and ensembling are all ways to improve the\n",
        "performance of decision trees.Boosting is a method of iteratively\n",
        "improving a model. One benefit of boosting is that both the bias and\n",
        "variance of the model is controlled, as opposed to just one. There are\n",
        "several popular algorithms for boosting: AdaBoost, Gradient Boosting,\n",
        "XGBoost, and others. One of the issues with most boosting algorithms is\n",
        "the computation time, which is why the XGBoost algorithm has gained so\n",
        "much traction over recent years.From Tianqi Chen and Carlos Guestrin in\n",
        "March of 2014, XGBoost has been a monumental system in the field of\n",
        "advanced analytics and machine learning. XGBoost was initially a\n",
        "research project started by Tianqi Chen - and later Carlos Guestrin - at\n",
        "the University of Washington, who presented their paper at SIGKDD\n",
        "Conference in 2016. Since then, version 1.5.0 is available for public\n",
        "usage and has been implemented in more than 5 languages including: C++,\n",
        "Python, R, Java, Scala, and Julia. In addition, the system is available\n",
        "to all modern operating systems, including: Windows, OS X, Linux, and a\n",
        "variety of cloud platforms.\n",
        "\n",
        "# **Background**\n",
        "\n",
        "## **Decision Trees**\n",
        "\n",
        "Decision Trees are simple clustering algorithms that split the predictor\n",
        "space into distinct, non-overlapping regions.Trees are easy to\n",
        "visualize, are flexible since they make no assumption about the\n",
        "functional form, and can model highly non-linear relationships.They can\n",
        "also be used in the context of regression and classification.\n",
        "\n",
        "Generally speaking, the algorithm works by searching through the\n",
        "available predictors and selecting the one that splits the leads to the\n",
        "greatest reduction in residual sums of squares (RSS), called recursive\n",
        "binary splitting.However, the model does not look beyond a single split\n",
        "to see if other predictors may lead to a greater overall reduction of\n",
        "RSS over the course of many splits.This is called a greedy algorithm.\n",
        "Considering the image below, we see that decision trees are a top-down\n",
        "approach, meaning that they start with the full predictor space then,\n",
        "moving down, create rules that split the space into distinct regions.\n",
        "The end result is a set divided predictor space of non-overlapping\n",
        "regions, as seen on the left. The splitting process continues until a\n",
        "criterion is met, whether it be a defined maximal depth, number of\n",
        "observations per leaf, or number of observations per split.\n",
        "\n",
        "<figure>\n",
        "<img src=\"attachment:TreeBasedDesign.png\"\n",
        "alt=\"Bishop 2006 from 2 Charles U ML reference\" />\n",
        "<figcaption aria-hidden=\"true\">Bishop 2006 from 2 Charles U ML\n",
        "reference</figcaption>\n",
        "</figure>\n",
        "\n",
        "Tree pruning is a method that can improve the predictive accuracy of\n",
        "decision trees. Pruning refers to the reduction in depth of the tree.\n",
        "One concern about decision trees is over-fitting by modeling noise or\n",
        "being too complex. Utilizing a process known as cost complexity pruning,\n",
        "the optimal tree depth may be determined.This process considers the\n",
        "prediction error and the number of terminal nodes, as illustrated in the\n",
        "equation below.Cost complexity pruning aims to minimize this equation.\n",
        "The tuning parameter, $\\alpha$, controls the complexity, or depth, of\n",
        "the tree. A large $\\alpha$ will result in a deep, complex tree (less\n",
        "pruning) and a small will result in a shallow tree (more pruning). In\n",
        "general, shallow trees cannot model all of the patterns in the data and\n",
        "will have higher prediction error at the cost of low complexity.Deep\n",
        "trees can model the patterns in the data at the cost of high complexity\n",
        "and potential over-fitting.There exists a happy medium between the two.\n",
        "\n",
        "Classification requires small modifications to the fitting process\n",
        "compared to regression. Notably, instead of considering RSS,\n",
        "classification trees may consider either classification error, entropy,\n",
        "or the Gini Index.When the goal of modelling is prediction accuracy, it\n",
        "is generally advisable to minimize classification error for the splits.\n",
        "Classification error looks at the proportion of observations in each\n",
        "region belonging to a certain class and compares this to the most common\n",
        "classification of observations in that region. The equation below is\n",
        "used to calculate the classification error given region $m$ and class\n",
        "$k$. \n",
        "$$\n",
        "CE = 1 - max(\\hat{p}_{mk})\n",
        "$$\n",
        "\n",
        "Entropy and the Gini Index are most commonly used for classification and\n",
        "are very similar in that they consider node purity. Node purity refers\n",
        "to the assortment of observations in a node. If all of the observations\n",
        "in a node have the same class label, then it is said to have high\n",
        "purity. If there are many different observation classes in a node, then\n",
        "it is not pure.High node purity is generally preferred. The equations\n",
        "for entropy and Gini Index are below.Just as regression trees aim to\n",
        "minimize RSS, classification trees aim to minimize entropy and Gini.\n",
        "\n",
        "$$\n",
        "G = \\Large \\sum_{k=1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})\n",
        "$$\n",
        "\n",
        "$$\n",
        "E = \\Large - \\sum_{k=1}^{K} \\hat{p}_{mk} log(\\hat{p}_{mk})\n",
        "$$\n",
        "\n",
        "Decision trees can be advantageous since they are easy to interpret and\n",
        "can model a variety of data types. However, they have some weaknesses. A\n",
        "primary weakness is that they can have a high degree of variance.Another\n",
        "weakness is that they are not robust in the sense that the\n",
        "addition/removal of a single observation may result in a very different\n",
        "tree shape.There are several ways to handle these concerns, commonly\n",
        "bagging, random forests, and boosting. We will discuss boosting, in\n",
        "particular.\n",
        "\n",
        "## Boosting\n",
        "\n",
        "Boosting is a general ensemble algorithm that can be used to improve the\n",
        "predictive accuracy of decision trees. Boosting assumes a generic\n",
        "function then fits a shallow tree of the residuals. After incorporating\n",
        "the new tree to get an updated function, another shallow tree is fit to\n",
        "the updated function’s residuals.This process repeats until a criterion\n",
        "is met. Just like decision trees, the final resulting model is a single\n",
        "tree.\n",
        "\n",
        "The boosting algorithm has three main steps:\n",
        "\n",
        "1.  Define an initial model to be fit to the data.\n",
        "\n",
        "2.  Iteratively fit B-many shallow trees to the residuals of the\n",
        "    previous model and add them to the previous model\n",
        "\n",
        "    1.  Fit a tree with d-many splits to the prior model’s residuals\n",
        "\n",
        "    2.  Update the previous model by adding a shrunken version of the\n",
        "        shallow tree.\n",
        "\n",
        "    3.  Update the residuals by subtracting the shrunken version of the\n",
        "        shallow tree\n",
        "\n",
        "3.  After B-many shallow trees have been fit to the residuals, the\n",
        "    result is a single boosted model.\n",
        "\n",
        "From the algorithm, we can see that the boosted model is a single tree\n",
        "that consists of a collection of weak learners (shallow trees). It is\n",
        "powerful because it can control for both bias and variance. Each\n",
        "addition of a weak learner aims to reduce the bias of the model. The\n",
        "collection of weak learners has the effect of reducing the variance of\n",
        "the model. Even though it controls for both, most of the focus is on\n",
        "bias reduction.If the initial function in the first step of the\n",
        "algorithm is $\\hat{f}_{x}$ , then the initial residuals are the\n",
        "y-values.When this is the case, the final boosted model will be the\n",
        "following equation, where $\\hat{f_{b}}(x)$ is the weak-learning decision\n",
        "tree from each iteration.\n",
        "\n",
        "$$\n",
        "\\Large \\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f_{b}}(x)\n",
        "$$  \n",
        "There are three tuning parameters for boosting: B, , and d. The\n",
        "parameter B represents the number of shallow trees that will be fit to\n",
        "the residuals. This value should be determined using cross-validation\n",
        "since a large value may result in over-fitting. The parameter B is\n",
        "closely tied to the shrinkage parameter . The parameter is called the\n",
        "shrinkage parameter and this controls the rate at which the model\n",
        "learns. A small lambda value confers a small adjustment to each\n",
        "sequential model. If the value of is extremely small, the model will\n",
        "need a large value for B in order for the data to be fit accurately. The\n",
        "parameter d defines the number of splits in each of the shallow trees.\n",
        "When we are referring to these sequentially added shallow trees, we mean\n",
        "that the value of d is small.\n",
        "\n",
        "There are several algorithms for implementing boosting. Some example\n",
        "algorithms are AdaBoost, Gradient Boosting, and XGBoost (there are\n",
        "others). AdaBoost is commonly recognized as the first algorithm that\n",
        "successfully implemented the boosting algorithm. Gradient Boosting is a\n",
        "generalization of AdaBoost, and XGBoost is an optimized gradient\n",
        "boosting algorithm. One problem with boosting is that it can be very\n",
        "slow, especially for large datasets. This is part of the reason why\n",
        "XGBoost has become so popular.\n",
        "\n",
        "### XGBoost\n",
        "\n",
        "There are many algorithms to implement boosting. The first boosting\n",
        "algorithm is recognized as AdaBoost and was mostly used for binary\n",
        "classification. Gradient Boosting is a generalization of AdaBoost and\n",
        "allows for the use of different loss functions. This improvement meant\n",
        "that it could be applied to regression and multi-class classification\n",
        "problems.One issue with Gradient Boosting, which is common among\n",
        "boosting in general, is that the learning process can be very\n",
        "slow.XGBoost is an improved implementation of the Gradient Boosting\n",
        "algorithm optimized for speed and accuracy.It is optimized for speed and\n",
        "has been observed to be more accurate in many cases, which is how it got\n",
        "its name: eXtreme Gradient Boosting. The success of XGBoost is\n",
        "attributed to several key algorithmic improvements:  \n",
        "\n",
        "1.  Regularized learning objective\n",
        "\n",
        "2.  Novel tree-based algorithm for handling sparse data\n",
        "\n",
        "3.  Weighted quantile sketch for learning\n",
        "\n",
        "4.  Optimized computer hardware utilization\n",
        "\n",
        "To implement a boosting algorithm, there is a chosen loss function\n",
        "(usually based on prediction error) that gets minimized. Gradient\n",
        "Boosting uses gradients to accomplish this. XGBoost improves upon this\n",
        "process by\n",
        "\n",
        "  \n",
        "XGBoost uses regularization of the loss function. Briefly, boosting\n",
        "works by iteratively adding weak learner shallow trees. This improvement\n",
        "helps to prevent over fitting by smoothing the solutions of terminal\n",
        "nodes in the shallow trees that are used for boosting.Regularization\n",
        "works by adding a penalty to a traditional loss function, usually based\n",
        "on errors or residuals. Common types of regularization are the l1 and l2\n",
        "regularization.The loss function used by XGBoost is given below.The\n",
        "first term is a loss function that measures the difference between the\n",
        "predicted and actual values of y.The second term is the regularization\n",
        "term, or penalty, and uses both l1 and l2 regularization.The penalty\n",
        "works by reducing the complexity of the model.This rationale is\n",
        "analogous to cost complexity pruning in decision trees.There are\n",
        "alternative methods for regularization, but this method has been shown\n",
        "to be easier to implement for parallelization.Note that when the\n",
        "regularization term is set to zero, the loss function is identical to\n",
        "traditional gradient boosting.  \n",
        "\n",
        "$$\n",
        "\\Large L = \\sum_{i}l(y_{i'}, \\hat{y_{i}}) + \\sum_{k}(\\gamma T + \\frac {1}{2} \\lambda ||w||^{2})\n",
        "$$\n",
        "\n",
        "The second major improvement on boosting is the use of second-order\n",
        "derivatives in the optimization of the loss function.The use of\n",
        "second-derivatives yields more information about how to minimize the\n",
        "loss function faster.Usually for large datasets, it is not possible to\n",
        "explore all possible tree structures. Therefore, the use of the second\n",
        "derivative helps to pick which splits are optimal.The loss function\n",
        "cannot be optimized using traditional methods.The designers of XGBoost\n",
        "proposed an additive training strategy that necessitates the use of the\n",
        "second derivative for large datasets.\n",
        "\n",
        "A third major feature is shrinkage and feature sub-sampling to prevent\n",
        "over-fitting.Shrinkage refers to the parameter used in the boosting\n",
        "algorithm described in the previous section. Feature sub-sampling is a\n",
        "technique that refers to the construction of the shallow trees during\n",
        "boosting.When these trees are constructed, at each split, only a\n",
        "fraction of the total features are considered.Random forests are known\n",
        "for employing this methodology as it decorrelates successive trees and\n",
        "allows for different patterns to be captured.\n",
        "\n",
        "There are several splitting algorithms incorporated into the function\n",
        "that can be defined by the user.These splitting algorithms are listed\n",
        "below.\n",
        "\n",
        "1.  Exact Greedy Algorithm: This algorithm considers all of the possible\n",
        "    splits for the features and picks a split based on the greatest\n",
        "    reduction in the loss function.For continuous variables, the\n",
        "    observations are sorted in ascending order then calculates split\n",
        "    statistics (such as Sum of Squared Errors)between each\n",
        "    observation.For n-many observations and m-many features, there are\n",
        "    (n-1) possible splits per feature and m(n-1) possible splits. For\n",
        "    large datasets, this is infeasible and requires the use of the\n",
        "    approximate algorithm.\n",
        "\n",
        "2.  Approximate Algorithm: This algorithm improves upon the exact greedy\n",
        "    algorithm and can work with large datasets.Instead of exploring all\n",
        "    possible splits, the algorithm proposes split points based on\n",
        "    quantiles and maps continuous features to these “bins”.Split\n",
        "    statistics are calculated for the bins as a whole and then the split\n",
        "    point is chosen based on which was best. The quantile strategy is\n",
        "    distributable and recomputable, meaning it is faster.The designers\n",
        "    also showed that this approximate quantile strategy can yield\n",
        "    similar predictive performance as models that use the exact\n",
        "    strategy. This quantile strategy is called weighted quantile sketch,\n",
        "    and can be used for weighted data.\n",
        "\n",
        "3.  Sparsity-Aware Split Finding: Many datasets contain sparse data\n",
        "    (such as missing data and zero-entries from one-hot encoding or\n",
        "    other values). By incorporating a pattern recognition algorithm for\n",
        "    sparse data, the model can run much faster for datasets with lots of\n",
        "    sparse data.XGBoost assigns a default direction to each node when it\n",
        "    is sparse.This “unified” approach allows for faster computation\n",
        "    times.  \n",
        "\n",
        "The final improvement for XGBoost is the system optimization.Tree\n",
        "construction and sorting data (during the splitting algorithm) are the\n",
        "most computationally intensive and time consuming. Large datasets may\n",
        "not always be able to be modeled on a device. XGBoost was designed to be\n",
        "able to work efficiently on any device and in parallel with other\n",
        "devices in a distributed manner.To take the most advantage of a computer\n",
        "system, large datasets need to first be divided into blocks of data.\n",
        "XGBoost utilizes the following computational system improvements:\n",
        "\n",
        "1.  Column Block for Parallel Learning - To reduce the time of sorting,\n",
        "    data is stored in blocks and only sorted once. The data is stored in\n",
        "    a compressed column format.\n",
        "\n",
        "2.  Parallelization - The data blocks are distributed among the CPU\n",
        "    cores. Therefore, collecting statistics from columns can be done\n",
        "    using a parallel algorithm for finding splits.\n",
        "\n",
        "3.  Distributed Computing - Blocking the data allows for it to be\n",
        "    distributed among different machines or disks. This allows the model\n",
        "    to work on large datasets.\n",
        "\n",
        "4.  Cache Awareness - Storing gradient statistics in the cache of a CPU\n",
        "    is computationally more efficient.\n",
        "\n",
        "5.  Blocks for Out-of-Core Computation - Dividing the data into blocks\n",
        "    and using computer science techniques known as block compression and\n",
        "    sharding allows for the model to work with very large datasets.\n",
        "\n",
        "In summary, XGBoost is a gradient boosting algorithm that is designed\n",
        "for speed and scalability.It incorporates many different algorithms that\n",
        "the user can choose to use depending on their data structure.XGBoost is\n",
        "commonly used to take advantage of these design features and has been\n",
        "shown to perform just as well, oftentimes better, than other gradient\n",
        "boosting algorithms\n",
        "\n",
        "## Application: Forest Cover Type\n",
        "\n",
        "Given forestry data from four wilderness areas in Roosevelt National\n",
        "Forest, classify the patches into one of 7 cover types, listed below:\n",
        "\n",
        "  \n",
        "1. Spruce/fir\n",
        "\n",
        "1.  Lodgepole Pine\n",
        "\n",
        "2.  Ponderosa Pine\n",
        "\n",
        "3.  Cottonwood/Willow\n",
        "\n",
        "4.  Aspen\n",
        "\n",
        "5.  Douglas/fir\n",
        "\n",
        "6.  Krummholz\n",
        "\n",
        "### Data Description\n",
        "\n",
        "The forest cover type problem requires a prediction of the type of trees\n",
        "that are growing on a plot of land from a variety of descriptive\n",
        "features that affect which species are able to grow in those\n",
        "conditions.An effective model would be able to accurately predict the\n",
        "cover type, allowing researchers to make these predictions without using\n",
        "remotely sensed data.There are 12 predictor variables that can be used\n",
        "to predict the cover type.Table 1 below summarizes each of these\n",
        "predictors. Each observation in the dataset consists of measurements\n",
        "from these variables from a 30 meter by 30 meter plot of land in\n",
        "northern Colorado.There are 581,012 observations in this dataset.The\n",
        "dataset was released in 1998.\n",
        "\n",
        "| Variable Name                      | Type         | Measurement Unit        | Description                                             |\n",
        "|----------------|---------------|---------------|--------------------------|\n",
        "| Elevation                          | Quantitative | Meters                  | Elevation in Meters                                     |\n",
        "| Aspect                             | Quantitative | Azimuthal Angle         | Aspect in Degrees Azimuth                               |\n",
        "| Slope                              | Quantitative | Degrees                 | Slope in Degrees                                        |\n",
        "| Horizontal Distance to Hydrology   | Quantitative | Meters                  | Horizontal distance to nearest surface water features   |\n",
        "| Vertical Distance to Hydrology     | Quantitative | Meters                  | Vertical distance to nearest surface water features     |\n",
        "| Horizontal Distance to Roadways    | Quantitative | Meters                  | Horizontal distance to nearest roadway                  |\n",
        "| Hillshade 9 AM                     | Quantitative | 0-255 Index             | Hillshade index at 9 AM, during summer solstice         |\n",
        "| Hillshade Noon                     | Quantitative | 0-255 Index             | Hillshade index at noon, during summer solstice         |\n",
        "| Hillshade 3 PM                     | Quantitative | 0-255 Index             | Hillshade index at 3 PM, during summer solstice         |\n",
        "| Horizontal Distance to Fire Points | Quantitative | Meters                  | Horizontal distance to nearest wildfire ignition points |\n",
        "| Wilderness Area                    | Qualitative  | 0 (absent), 1 (present) | Wilderness area designation                             |\n",
        "| Soil Type                          | Qualitative  | 0 (absent), 1 (present) | Soil type designation                                   |\n",
        "| Cover Type                         | Integer      | 1 to 7                  | Forest cover type designation                           |\n",
        "\n",
        "Table 1: Above is a summary of all of the features in the dataset. The\n",
        "dataset contains a total of 54 features because some of the variables\n",
        "are one-hot encoded, however, there are only 13 truly unique features.\n",
        "\n",
        "The data was collected from four different wilderness areas (out of a\n",
        "total 6 wilderness areas) within the Roosevelt National Forest in\n",
        "Northern Colorado.The cover type was determined by the US Forest Service\n",
        "Region 2 Resource Information System data. A wilderness area is an area\n",
        "that is relatively untouched by humans; this means that any ecological\n",
        "processes are the result of nature rather than forest management\n",
        "services. The four wilderness areas studied in Roosevelt National Forest\n",
        "were Neota, Rawah, Comanche, and Cache la Poudre. These four areas\n",
        "differ in elevation and geography, even though they are nearby,\n",
        "resulting in different species of trees covering the land.Neota consists\n",
        "mostly of spruce/fir.Rawah and Comanche are mostly lodgepole pine, with\n",
        "a smattering of spruce/fir and aspen. Cache la Poudre consists mostly of\n",
        "Ponderosa Pine, Douglas/Fir, and Cottonwood/Willow. It should be noted\n",
        "that approximately 85% of the observations consist of two types of\n",
        "classes.There are no missing values.\n",
        "\n",
        "Tables 1-2 and Figure 1 show some frequency distributions of the cover\n",
        "types and wilderness areas. From these, it can be seen that the dataset\n",
        "is dominated by the two cover types and two wilderness areas. This is\n",
        "attributable to the relative sizes of the wilderness areas. It is\n",
        "important to note the imbalance of observations as this may impact the\n",
        "bias of the model. The Rawah wilderness area is approximately 119.4 sq.\n",
        "miles; the Neota wilderness area is approximately 15.5 sq. miles; the\n",
        "Comanche wilderness area is approximately 104.4 sq. miles; the Cache la\n",
        "Poudre wilderness area is approximately 14.5 sq. miles. From Figure 1,\n",
        "there is a significant proportion of the total observations\n",
        "(approximately 20%) that are in the same soil type and cover type.\n",
        "Figure 2 displays a matrix of correlation values between all of the\n",
        "numeric variables. No significant conclusions regarding the relationship\n",
        "between variables can be concluded. Figure 3 shows that elevation does a\n",
        "great job distinguishing between the cover types. Other variables were\n",
        "explored, but there were no significant patterns identified.\n",
        "\n",
        "| Class | Cover Type        | Frequency | Relative Frequency |\n",
        "|-------|-------------------|-----------|--------------------|\n",
        "| 1     | Spruce/fir        | 211840    | 0.365              |\n",
        "| 2     | Lodgepole Pine    | 283301    | 0.488              |\n",
        "| 3     | Ponderosa Pine    | 35754     | 0.062              |\n",
        "| 4     | Cottonwood/Willow | 2747      | 0.005              |\n",
        "| 5     | Aspen             | 9493      | 0.016              |\n",
        "| 6     | Douglas/fir       | 17367     | 0.03               |\n",
        "| 7     | Krummholz         | 20510     | 0.035              |\n",
        "\n",
        "Table 2: A table of observation frequencies shows that the observations\n",
        "are dominated by Spruce/Fir and Lodgepole Pine.\n",
        "\n",
        "| Wilderness Area | Frequency |\n",
        "|-----------------|-----------|\n",
        "| Rawah           | 260796    |\n",
        "| Neota           | 29884     |\n",
        "| Comanche Peak   | 253364    |\n",
        "| Cache la Poudre | 36968     |\n",
        "\n",
        "Table 3: A table of wilderness area frequencies shows that approximately\n",
        "88% of the observations came from two of the four areas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
